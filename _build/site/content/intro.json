{"version":2,"kind":"Article","sha256":"4b30c889fef4bd4c3ac94e173cf7cced7cdf012d2d8bab6c3d53809b8f81dbbc","slug":"intro","location":"/content/intro.md","dependencies":[],"frontmatter":{"authors":[{"nameParsed":{"literal":"Lune P Bellec","given":"Lune P","family":"Bellec"},"name":"Lune P Bellec","orcid":"0000-0002-9111-0699","corresponding":false,"equal_contributor":false,"email":"lune.bellec@umontreal.ca","roles":["Conceptualization","Software","Writing – original draft"],"affiliations":["udem"],"url":"https://github.com/pbellec","github":"lbellec","id":"contributors-myst-generated-uid-0"},{"nameParsed":{"literal":"Sara Barbu","given":"Sara","family":"Barbu"},"name":"Sara Barbu","orcid":"0009-0009-1808-375X","equal_contributor":false,"email":"sara.barbu@umontreal.ca","roles":["Software","Writing – review & editing"],"affiliations":["udem"],"github":"SaraBarbu","id":"contributors-myst-generated-uid-1","corresponding":true}],"doi":"10.55458/neurolibre.xxxxx","open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/courtois-neuromod/image10k.paper","subject":"Living Preprint","venue":{"title":"Neurolibre"},"numbering":{"heading_1":{"enabled":true},"heading_2":{"enabled":true},"heading_3":{"enabled":true},"heading_4":{"enabled":true},"heading_5":{"enabled":true},"heading_6":{"enabled":true}},"keywords":["image stimuli","annotation","crowdsourcing"],"affiliations":[{"id":"udem","name":"Université de Montréąl","department":"Psychology","address":"2900 Bd Édouard-Montpetit","city":"Montreal","state":"Quebec","postal_code":"H3T 1J4","country":"Canada","isni":"0000 0004 0435 3292","ror":"https://ror.org/0161xgx34","phone":"(514) 343-6111"}],"abbreviations":{"MyST":"Markedly Structured Markdown"},"edit_url":"https://github.com/courtois-neuromod/image10k.paper/blob/main/content/intro.md","exports":[{"format":"md","filename":"intro.md","url":"/intro-740dfb4cd156716735b16efdd0e13f6b.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Visual object recognition entails the interpretation of complex, multidimensional semantic information from natural scenes (e.g., an object’s typical appearance, function, and context) and is a central area of investigation in both cognitive neuroscience and artificial intelligence. To enable direct comparison of human and artificial visual processing, and to support the development of computational models of human visual understanding, a critical challenge is the creation of image benchmarks with sufficient scale, ecological validity, and behavioral grounding. Here we present [Dataset Name], a dataset designed to probe visual semantic knowledge through 8,382 naturalistic images spanning over 15 distinct semantic categories. These images feature objects within their typical environmental contexts, and their descriptive labels and category assignments have been rigorously validated through online behavioral experiments. We anticipate that this openly available, large-scale dataset of ecologically valid images will serve as a valuable benchmark for both computational and neuroscientific investigations of visual semantic knowledge and object recognition.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NFx2tXCsg4"}],"key":"Du8r9XmSge"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"A common practice in psychology and cognitive science has been the use of highly controlled, often artificial stimuli, typically encountered exclusively within laboratory settings. For instance, numerous vision studies have employed images depicting objects isolated from their natural backgrounds, creating a significant discrepancy between experimental stimuli and the rich, contextualized visual environments humans experience daily. This methodological choice not only limits the ecological validity of findings but also presents challenges for computational modeling. The relatively small scale of stimulus sets typically used in human cognitive neuroscience, compared to those employed in machine learning, can impair the development and training of artificial neural networks aiming to model perceptual mechanisms from human brain activity. Furthermore, copyright restrictions often limit the accessibility of image sets used in published research, hindering reproducibility and collaborative efforts.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oRxFyHZCXH"}],"key":"HxnQL82j5T"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"In recent years, there has been a growing trend towards utilizing larger and more naturalistic object image datasets in cognitive neuroscience. However, existing resources present their own limitations. Classical databases of object concepts often rely on manually curated, and thus potentially limited or biased, sets of concepts. Similarly, databases of naturalistic object images may consist of objects cropped from their context, or conversely, comprise vast collections of images with variable quality, necessitating extensive manual curation.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"xwObefSSUb"}],"key":"U9DvMJpdIT"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The investigation of semantic knowledge—how humans and artificial systems represent and process meaning (e.g., a lemon’s color, flavor, and typical use)—is a central theme in both cognitive neuroscience and artificial intelligence. To facilitate direct comparisons between human and artificial semantic representations, and to support the advancement of natural language processing (NLP) in modeling human understanding, the development of large-scale, robust benchmarks is critical. Such benchmarks, particularly those incorporating rich visual information, are essential for bridging these disciplines.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"bLwAA6AZeb"}],"key":"jahUfafCbr"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Here, we introduce [Dataset Name], a novel dataset comprising 8,382 high-quality, naturalistic images spanning over 15 distinct semantic categories. A key feature of this dataset is its rigorous behavioral validation: using crowdsourcing methodologies, we have confirmed the accuracy of object labels and their corresponding category assignments for each image. The images depict objects within their natural contexts, enhancing ecological validity.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"dSuTQBLL8W"}],"key":"Y0NQlSD2KS"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"[Dataset Name] is designed to address several of the aforementioned limitations. It offers a large-scale, openly available resource of ecologically valid visual stimuli. We anticipate that this dataset will serve as a valuable tool for researchers in psychology, neuroscience, and computer science. Specifically, it can support investigations into semantic knowledge representation, facilitate the training and benchmarking of computational models of vision and cognition, and promote reproducible research through its open accessibility. This resource aims to bridge the gap between systematic, controlled experimentation and the complexities of real-world perception and understanding.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"perznuzv2W"}],"key":"WJxbfRm0JT"}],"key":"EX4onIaZim"}],"key":"OuCp3mlX9t"},"references":{"cite":{"order":[],"data":{}}}}